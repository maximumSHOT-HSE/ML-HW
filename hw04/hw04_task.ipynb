{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[ML][HW04] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным пользователей социальной сети Вконтакте, и сравнить его эффективность с ансамблем, предоставляемым библиотекой CatBoost.\n",
    "\n",
    "В результате мы сможем определить, какие подписки пользователей больше всего влияют на определение возраста и пола человека. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import typing\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(x: np.ndarray):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    proba = counts / len(x)\n",
    "    return sum(proba * (1 - proba))\n",
    "\n",
    "\n",
    "def entropy(x: np.ndarray):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    proba = counts / len(x)\n",
    "    return -sum(proba * np.log2(proba))\n",
    "\n",
    "\n",
    "def gain(left_y: np.ndarray, right_y: np.ndarray, criterion):\n",
    "    y = np.concatenate((left_y, right_y))\n",
    "    return criterion(y) - (criterion(left_y) * len(left_y) + criterion(right_y) * len(right_y)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (2 балла)\n",
    "Random Forest состоит из деревьев решений. Каждое такое дерево строится на одной из выборок, полученных при помощи bagging. Элементы, которые не вошли в новую обучающую выборку, образуют out-of-bag выборку. Кроме того, в каждом узле дерева мы случайным образом выбираем набор из `max_features` и ищем признак для предиката разбиения только в этом наборе.\n",
    "\n",
    "Сегодня мы будем работать только с бинарными признаками, поэтому нет необходимости выбирать значение признака для разбиения.\n",
    "\n",
    "#### Методы\n",
    "`predict(X)` - возвращает предсказанные метки для элементов выборки `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`X, y` - обучающая выборка и соответствующие ей метки классов. Из нее нужно получить выборку для построения дерева при помощи bagging. Out-of-bag выборку нужно запомнить, она понадобится потом.\n",
    "\n",
    "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
    "\n",
    "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
    "\n",
    "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
    "\n",
    "`max_features=\"auto\"` - количество признаков, которые могут использоваться в узле. Если `\"auto\"` - равно `sqrt(X.shape[1])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_bag(size, ids: np.ndarray) -> np.ndarray:\n",
    "    used = set()\n",
    "    for i in ids:\n",
    "        used.add(i)\n",
    "    return np.array([i for i in range(size) if i not in used])\n",
    "\n",
    "\n",
    "def bagging(size: int) -> np.ndarray:\n",
    "    ids = []\n",
    "    for _ in range(size):\n",
    "        ids.append(np.random.randint(size))\n",
    "    return np.array(ids)\n",
    "\n",
    "\n",
    "class DecisionTreeLeaf:\n",
    "    def __init__(self, ys: np.ndarray):\n",
    "        occur = dict()\n",
    "        if ys.size > 0:\n",
    "            for y in ys:\n",
    "                if y in occur:\n",
    "                    occur[y] += 1\n",
    "                else:\n",
    "                    occur[y] = 1\n",
    "        for k in occur.keys():\n",
    "            occur[k] /= ys.shape[0]\n",
    "        self.probabilities = occur\n",
    "        self.y = max(self.probabilities.keys(), key=lambda label: self.probabilities[label])\n",
    "        self.size = ys.size\n",
    "\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, split_dim, left, right):\n",
    "        self.split_dim = split_dim\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(\n",
    "            self,\n",
    "            xs: np.ndarray,\n",
    "            ys: np.ndarray,\n",
    "            criterion: str = 'gini',\n",
    "            max_depth=None,\n",
    "            min_samples_leaf: int = 1,\n",
    "            max_features='auto'\n",
    "    ):\n",
    "        self.criterion = gini if criterion == 'gini' else entropy\n",
    "        self.max_depth = max_depth if max_depth else math.inf\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_feature = min(int(max_features), xs.shape[1]) if max_features != 'auto' \\\n",
    "            else math.ceil(xs.shape[1] ** 0.5)\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.bag = bagging(len(xs))\n",
    "        self.out_of_bag = out_of_bag(len(xs), self.bag)\n",
    "        self.root = self.build_tree(self.xs[self.bag], self.ys[self.bag])\n",
    "\n",
    "    def build_tree(self, xs: np.ndarray, ys: np.ndarray, depth: int = 0):\n",
    "        if np.unique(np.sort(ys)).size == 1 or depth == self.max_depth:\n",
    "            return DecisionTreeLeaf(ys)\n",
    "\n",
    "        best_dim = -1\n",
    "        best_ig = 0\n",
    "        ids = [i for i in range(xs.shape[0])]\n",
    "\n",
    "        dims = np.array([i for i in range(xs.shape[1])])\n",
    "        np.random.shuffle(dims)\n",
    "        dims = dims[:self.max_feature]\n",
    "\n",
    "        for dim in dims:\n",
    "            left_y = np.array([ys[ids[q]] for q in ids if xs[ids[q]][dim] == 0])\n",
    "            right_y = np.array([ys[ids[q]] for q in ids if xs[ids[q]][dim] == 1])\n",
    "\n",
    "            if left_y.shape[0] < self.min_samples_leaf or right_y.shape[0] < self.min_samples_leaf:\n",
    "                continue\n",
    "\n",
    "            ig = gain(left_y, right_y, self.criterion)\n",
    "\n",
    "            if best_dim == -1 or ig > best_ig:\n",
    "                best_ig = ig\n",
    "                best_dim = dim\n",
    "\n",
    "        if best_dim == -1:\n",
    "            return DecisionTreeLeaf(ys)\n",
    "\n",
    "        left_xs = np.array([xs[ids[q]] for q in ids if xs[ids[q]][best_dim] == 0])\n",
    "        left_ys = np.array([ys[ids[q]] for q in ids if xs[ids[q]][best_dim] == 0])\n",
    "\n",
    "        right_xs = np.array([xs[ids[q]] for q in ids if xs[ids[q]][best_dim] == 1])\n",
    "        right_ys = np.array([ys[ids[q]] for q in ids if xs[ids[q]][best_dim] == 1])\n",
    "\n",
    "        left_son = self.build_tree(left_xs, left_ys, depth + 1)\n",
    "        right_son = self.build_tree(right_xs, right_ys, depth + 1)\n",
    "\n",
    "        return DecisionTreeNode(best_dim, left_son, right_son)\n",
    "\n",
    "    def get_probabilities(self, x: np.ndarray, node) -> dict:\n",
    "        if isinstance(node, DecisionTreeLeaf):\n",
    "            return node.probabilities\n",
    "        if x[node.split_dim] == 0:\n",
    "            return self.get_probabilities(x, node.left)\n",
    "        else:\n",
    "            return self.get_probabilities(x, node.right)\n",
    "\n",
    "    def predict_probabilities(self, xs: np.ndarray) -> typing.List[dict]:\n",
    "        return [self.get_probabilities(x, self.root) for x in xs]\n",
    "\n",
    "    def predict(self, xs: np.ndarray) -> np.ndarray:\n",
    "        probabilities = self.predict_probabilities(xs)\n",
    "        return np.array([max(p.keys(), key=lambda k: p[k]) for p in probabilities])\n",
    "\n",
    "    def out_of_bag_error(self):\n",
    "        xs = self.xs[self.out_of_bag]\n",
    "        ys = self.ys[self.out_of_bag]\n",
    "        y_pred = self.predict(xs)\n",
    "        err = sum(1 for i, y in enumerate(ys) if y != y_pred[i])\n",
    "        errors = []\n",
    "        for j in range(xs.shape[1]):\n",
    "            xsj = xs.copy()\n",
    "            np.random.shuffle(xsj[:, j])\n",
    "            y_pred = self.predict(xsj)\n",
    "            err_j = sum(1 for i, y in enumerate(ys) if y != y_pred[i])\n",
    "            errors.append((err_j - err) / len(ys))\n",
    "        return np.array(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority(a: np.ndarray):\n",
    "    count = dict()\n",
    "    for x in a:\n",
    "        if x in count:\n",
    "            count[x] += 1\n",
    "        else:\n",
    "            count[x] = 1\n",
    "    return max(count.keys(), key=lambda k: count[k])\n",
    "\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(\n",
    "            self,\n",
    "            criterion: str = 'gini',\n",
    "            max_depth: int = None,\n",
    "            min_samples_leaf: int = 1,\n",
    "            max_features='auto',\n",
    "            n_estimators: int = 10\n",
    "    ):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.forest: typing.List[DecisionTree] = []\n",
    "\n",
    "        self.xs = None\n",
    "        self.ys = None\n",
    "\n",
    "    def fit(self, xs: np.ndarray, ys: np.ndarray):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.forest.clear()\n",
    "        for _ in range(self.n_estimators):\n",
    "            self.forest.append(\n",
    "                DecisionTree(\n",
    "                    xs,\n",
    "                    ys,\n",
    "                    self.criterion,\n",
    "                    self.max_depth,\n",
    "                    self.min_samples_leaf,\n",
    "                    self.max_features\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def predict(self, xs: np.ndarray) -> np.ndarray:\n",
    "        votes = np.array([tree.predict(xs) for tree in self.forest])\n",
    "        return np.array([majority(votes[:, i]) for i in range(votes.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла)\n",
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest - посчитать out-of-bag ошибку предсказания `err_oob`, а затем перемешать значения признака `j` и посчитать ее (`err_oob_j`) еще раз. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(rfc: RandomForestClassifier):\n",
    "    return sum(tree.out_of_bag_error() for tree in rfc.forest) / rfc.n_estimators\n",
    "\n",
    "\n",
    "def most_important_features(importance, names, k: int = 20):\n",
    "    idicies = np.argsort(importance)[::-1][:k]\n",
    "    return np.array(names)[idicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, пришло время протестировать наше дерево на простом синтетическом наборе данных. В результате должна получиться точность `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Importance: [-1.28917217e-04 -3.96316810e-04  2.00867774e-01  2.00133029e-01\n",
      "  3.91425025e-01 -2.02179873e-04]\n"
     ]
    }
   ],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3,\n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "print(\"Importance:\", feature_importance(rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (1 балл)\n",
    "Теперь поработаем с реальными данными.\n",
    "\n",
    "Выборка состоит из публичных анонимизированных данных пользователей социальной сети Вконтакте. Первые два столбца отражают возрастную группу (`zoomer`, `doomer` и `boomer`) и пол (`female`, `male`). Все остальные столбцы являются бинарными признаками, каждый из них определяет, подписан ли пользователь на определенную группу/публичную страницу или нет.\\\n",
    "\\\n",
    "Необходимо обучить два классификатора, один из которых определяет возрастную группу, а второй - пол.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются. Лес должен строиться за какое-то разумное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    dataframe = pandas.read_csv(path, header=0)\n",
    "    dataset = dataframe.values.tolist()\n",
    "    random.shuffle(dataset)\n",
    "    y_age = [row[0] for row in dataset]\n",
    "    y_sex = [row[1] for row in dataset]\n",
    "    X = [row[2:] for row in dataset]\n",
    "    \n",
    "    return np.array(X), np.array(y_age), np.array(y_sex), list(dataframe.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset('src/resources/vk.csv')\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7099621689785625\n",
      "Most important features:\n",
      "1. 4ch\n",
      "2. styd.pozor\n",
      "3. ovsyanochan\n",
      "4. mudakoff\n",
      "5. rhymes\n",
      "6. rapnewrap\n",
      "7. dayvinchik\n",
      "8. tumblr_vacuum\n",
      "9. bot_maxim\n",
      "10. pravdashowtop\n",
      "11. pixel_stickers\n",
      "12. iwantyou\n",
      "13. bog_memes\n",
      "14. xfilm\n",
      "15. reflexia_our_feelings\n",
      "16. ultrapir\n",
      "17. thesmolny\n",
      "18. ohhluul\n",
      "19. ne.poverish\n",
      "20. tnt\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=11, max_features=13)\n",
    "rfc.fit(X_train, y_age_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8675914249684742\n",
      "Most important features:\n",
      "1. 40kg\n",
      "2. zerofat\n",
      "3. girlmeme\n",
      "4. 4ch\n",
      "5. mudakoff\n",
      "6. femalemem\n",
      "7. modnailru\n",
      "8. be.beauty\n",
      "9. thesmolny\n",
      "10. soverwenstvo.decora\n",
      "11. i_d_t\n",
      "12. sh.cook\n",
      "13. rapnewrap\n",
      "14. woman.blog\n",
      "15. reflexia_our_feelings\n",
      "16. decadence\n",
      "17. 9o_6o_9o\n",
      "18. bon\n",
      "19. igm\n",
      "20. psy.people\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=13, min_samples_leaf=4, max_features=15)\n",
    "rfc.fit(X_train, y_sex_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost\n",
    "В качестве аьтернативы попробуем CatBoost. \n",
    "\n",
    "Устаниовить его можно просто с помощью `pip install catboost`. Туториалы можно найти, например, [здесь](https://catboost.ai/docs/concepts/python-usages-examples.html#multiclassification) и [здесь](https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb). Главное - не забудьте использовать `loss_function='MultiClass'`.\\\n",
    "\\\n",
    "Сначала протестируйте CatBoost на синтетических данных. Выведите точность и важность признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "[8.93395675e-04 3.54998274e-04 2.78669154e+01 2.78635189e+01\n",
      " 4.42676096e+01 7.07666981e-04]\n"
     ]
    }
   ],
   "source": [
    "train_data, label_values = synthetic_dataset(1000)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    loss_function='MultiClass'\n",
    ")\n",
    "model.fit(\n",
    "    train_data,\n",
    "    label_values, \n",
    "    verbose=False\n",
    ")\n",
    "print(\"Accuracy:\", np.mean(model.predict(train_data).reshape(-1) == label_values))\n",
    "print(model.get_feature_importance(Pool(train_data, label_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (3 балла)\n",
    "Попробуем применить один из используемых на практике алгоритмов. В этом нам поможет CatBoost. Также, как и реализованный ними RandomForest, применим его для определения пола и возраста пользователей сети Вконтакте, выведите названия наиболее важных признаков так же, как в задании 3.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"src/resources/vk.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)\n",
    "X_train, X_eval, y_age_train, y_age_eval, y_sex_train, y_sex_eval = train_test_split(X_train, y_age_train, y_sex_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7301387137452712\n",
      "Most important features:\n",
      "1. ovsyanochan\n",
      "2. styd.pozor\n",
      "3. rhymes\n",
      "4. leprum\n",
      "5. 4ch\n",
      "6. mudakoff\n",
      "7. dayvinchik\n",
      "8. tumblr_vacuum\n",
      "9. pravdashowtop\n",
      "10. rapnewrap\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(\n",
    "    loss_function='MultiClass',\n",
    "    bootstrap_type='Bayesian',\n",
    "    bagging_temperature=0.1,\n",
    ")\n",
    "model.fit(\n",
    "    X=X_train,\n",
    "    y=y_age_train, \n",
    "    verbose=False,\n",
    "    eval_set=(X_eval, y_age_eval)\n",
    ")\n",
    "print(\"Accuracy:\", np.mean(model.predict(X_test).reshape(-1) == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(model.get_feature_importance(Pool(X_train, y_age_train)), features, 10)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8650693568726355\n",
      "Most important features:\n",
      "1. 40kg\n",
      "2. modnailru\n",
      "3. girlmeme\n",
      "4. i_d_t\n",
      "5. zerofat\n",
      "6. thesmolny\n",
      "7. femalemem\n",
      "8. academyofman\n",
      "9. beauty\n",
      "10. reflexia_our_feelings\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(\n",
    "    loss_function='MultiClass',\n",
    "    bootstrap_type='Bayesian',\n",
    "    bagging_temperature=0.1,\n",
    ")\n",
    "model.fit(\n",
    "    X=X_train,\n",
    "    y=y_sex_train, \n",
    "    verbose=False,\n",
    "    eval_set=(X_eval, y_sex_eval)\n",
    ")\n",
    "print(\"Accuracy:\", np.mean(model.predict(X_test).reshape(-1) == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(model.get_feature_importance(Pool(X_train, y_sex_train)), features, 10)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
